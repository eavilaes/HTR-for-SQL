{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainSimpleHTR.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"17C7825rDDh5gZL0_YqyZkK06m_geaHWR","authorship_tag":"ABX9TyONn4ZVdjiaj601l68cMoa8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NVoF8E5KqSpe"},"source":["#Integrate SimpleHTR with word beam search\r\n","Using CTCWordBeamSearch\r\n"]},{"cell_type":"code","metadata":{"id":"5HEUj2B-quNY"},"source":["!rm -rf CTCWordBeamSearch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvUn8qi2qe0w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615917093210,"user_tz":-60,"elapsed":17118,"user":{"displayName":"ERIC AVILA ESCUDERO","photoUrl":"","userId":"03170416067382264344"}},"outputId":"635ef376-4a30-4541-b65f-7da2e3fa5b34"},"source":["!git clone https://github.com/githubharald/CTCWordBeamSearch\r\n","%cd ./CTCWordBeamSearch\r\n","!pip install .\r\n","%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'CTCWordBeamSearch'...\n","remote: Enumerating objects: 84, done.\u001b[K\n","remote: Counting objects: 100% (84/84), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 340 (delta 19), reused 72 (delta 12), pack-reused 256\u001b[K\n","Receiving objects: 100% (340/340), 1.63 MiB | 25.62 MiB/s, done.\n","Resolving deltas: 100% (116/116), done.\n","/content/CTCWordBeamSearch\n","Processing /content/CTCWordBeamSearch\n","Building wheels for collected packages: word-beam-search\n","  Building wheel for word-beam-search (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word-beam-search: filename=word_beam_search-1.0.0-cp37-cp37m-linux_x86_64.whl size=1178307 sha256=78bc47f343ceb1e2923be32c52a71f07cdb2f2084838b73ee8a80c3fa9ff4359\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3czr_c8u/wheels/a9/69/4c/9d6acbecc7bf4b47c5072b213d9b08e4b9c43864bbed5206cc\n","Successfully built word-beam-search\n","Installing collected packages: word-beam-search\n","Successfully installed word-beam-search-1.0.0\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vl2zOsXpuHBt"},"source":["#Train the model with the dataset"]},{"cell_type":"code","metadata":{"id":"o-VhRUllG5Ho"},"source":["!cp '/content/drive/MyDrive/TFG/TrainSimpleHTR/dataset.zip' .\r\n","!unzip './dataset.zip'\r\n","!mv '/content/dataset/gt/wordsNew.txt' '/content/dataset/gt/words.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bt2xH8eZFWib"},"source":["!cp '/content/drive/MyDrive/TFG/TrainSimpleHTR/requirements.txt' .\r\n","!pip install -r requirements.txt\r\n","!pip install path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzuNRGS6FyHH"},"source":["!cp -r '/content/drive/MyDrive/TFG/TrainSimpleHTR/model' './model'\r\n","!cp -r '/content/drive/MyDrive/TFG/TrainSimpleHTR/data' './data'\r\n","!cp -r '/content/drive/MyDrive/TFG/TrainSimpleHTR/src' './src'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7N54KFXGHdO","executionInfo":{"status":"ok","timestamp":1615926683990,"user_tz":-60,"elapsed":637,"user":{"displayName":"ERIC AVILA ESCUDERO","photoUrl":"","userId":"03170416067382264344"}},"outputId":"6b5b2a12-16a6-4274-a1d2-749e75eba26c"},"source":["%cd ./src"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LpcRvY5x-5_9"},"source":["from pathlib import Path\r\n","\r\n","import argparse\r\n","import json\r\n","\r\n","import cv2\r\n","import editdistance\r\n","from path import Path\r\n","\r\n","from DataLoaderIAM import DataLoaderIAM, Batch\r\n","from Model import Model, DecoderType\r\n","from SamplePreprocessor import preprocess\r\n","\r\n","\r\n","class FilePaths:\r\n","    \"filenames and paths to data\"\r\n","    fnCharList = '../model/wordCharList.txt'\r\n","    fnSummary = '../model/summary.json'\r\n","    fnInfer = '../data/test.png'\r\n","    fnCorpus = '../data/corpus.txt'\r\n","\r\n","\r\n","def write_summary(charErrorRates, wordAccuracies):\r\n","    with open(FilePaths.fnSummary, 'w') as f:\r\n","        json.dump({'charErrorRates': charErrorRates, 'wordAccuracies': wordAccuracies}, f)\r\n","\r\n","\r\n","def train(model, loader):\r\n","    \"train NN\"\r\n","    epoch = 0  # number of training epochs since start\r\n","    summaryCharErrorRates = []\r\n","    summaryWordAccuracies = []\r\n","    bestCharErrorRate = float('inf')  # best valdiation character error rate\r\n","    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\r\n","    earlyStopping = 25  # stop training after this number of epochs without improvement\r\n","    while True:\r\n","        epoch += 1\r\n","        print('Epoch:', epoch)\r\n","\r\n","        # train\r\n","        print('Train NN')\r\n","        loader.trainSet()\r\n","        while loader.hasNext():\r\n","            iterInfo = loader.getIteratorInfo()\r\n","            batch = loader.getNext()\r\n","            loss = model.trainBatch(batch)\r\n","            print(f'Epoch: {epoch} Batch: {iterInfo[0]}/{iterInfo[1]} Loss: {loss}')\r\n","\r\n","        # validate\r\n","        charErrorRate, wordAccuracy = validate(model, loader)\r\n","\r\n","        # write summary\r\n","        summaryCharErrorRates.append(charErrorRate)\r\n","        summaryWordAccuracies.append(wordAccuracy)\r\n","        write_summary(summaryCharErrorRates, summaryWordAccuracies)\r\n","\r\n","        # if best validation accuracy so far, save model parameters\r\n","        if charErrorRate < bestCharErrorRate:\r\n","            print('Character error rate improved, save model')\r\n","            bestCharErrorRate = charErrorRate\r\n","            noImprovementSince = 0\r\n","            model.save()\r\n","        else:\r\n","            print(f'Character error rate not improved, best so far: {charErrorRate * 100.0}%')\r\n","            noImprovementSince += 1\r\n","            print('No improvement since: ' + str(noImprovementSince))\r\n","\r\n","        # stop training if no more improvement in the last x epochs\r\n","        if noImprovementSince >= earlyStopping:\r\n","            print(f'No more improvement since {earlyStopping} epochs. Training stopped.')\r\n","            break\r\n","\r\n","\r\n","def validate(model, loader):\r\n","    \"validate NN\"\r\n","    print('Validate NN')\r\n","    loader.validationSet()\r\n","    numCharErr = 0\r\n","    numCharTotal = 0\r\n","    numWordOK = 0\r\n","    numWordTotal = 0\r\n","    while loader.hasNext():\r\n","        iterInfo = loader.getIteratorInfo()\r\n","        print(f'Batch: {iterInfo[0]} / {iterInfo[1]}')\r\n","        batch = loader.getNext()\r\n","        (recognized, _) = model.inferBatch(batch)\r\n","\r\n","        print('Ground truth -> Recognized')\r\n","        for i in range(len(recognized)):\r\n","            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\r\n","            numWordTotal += 1\r\n","            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\r\n","            numCharErr += dist\r\n","            numCharTotal += len(batch.gtTexts[i])\r\n","            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gtTexts[i] + '\"', '->',\r\n","                  '\"' + recognized[i] + '\"')\r\n","\r\n","    # print validation result\r\n","    charErrorRate = numCharErr / numCharTotal\r\n","    wordAccuracy = numWordOK / numWordTotal\r\n","    print(f'Character error rate: {charErrorRate * 100.0}%. Word accuracy: {wordAccuracy * 100.0}%.')\r\n","    return charErrorRate, wordAccuracy\r\n","\r\n","\r\n","def infer(model, fnImg):\r\n","    \"recognize text in image provided by file path\"\r\n","    img = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\r\n","    batch = Batch(None, [img])\r\n","    (recognized, probability) = model.inferBatch(batch, True)\r\n","    print(f'Recognized: \"{recognized[0]}\"')\r\n","    print(f'Probability: {probability[0]}')\r\n","\r\n","\r\n","def main():\r\n","    # \"main function\"\r\n","    # parser = argparse.ArgumentParser()\r\n","    # parser.add_argument('--train', help='train the NN', action='store_true')\r\n","    args_train = False\r\n","    # parser.add_argument('--validate', help='validate the NN', action='store_true')\r\n","    args_validate = True\r\n","    # parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath', help='CTC decoder')\r\n","    args_decoder = 'wordbeamsearch'\r\n","    # parser.add_argument('--batch_size', help='batch size', type=int, default=100)\r\n","    args_batch_size = 100\r\n","    # parser.add_argument('--data_dir', help='directory containing IAM dataset', type=Path, required=False)\r\n","    args_data_dir = Path('/content/dataset')\r\n","    # parser.add_argument('--fast', help='use lmdb to load images', action='store_true')\r\n","    args_fast = False\r\n","    # parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\r\n","    args_dump = False\r\n","    # args = parser.parse_args()\r\n","\r\n","    if args_train and args_validate:\r\n","      print(\"Both Train and Validate are enabled.\")\r\n","      raise\r\n","\r\n","    # set chosen CTC decoder\r\n","    if args_decoder == 'bestpath':\r\n","        decoderType = DecoderType.BestPath\r\n","    elif args_decoder == 'beamsearch':\r\n","        decoderType = DecoderType.BeamSearch\r\n","    elif args_decoder == 'wordbeamsearch':\r\n","        decoderType = DecoderType.WordBeamSearch\r\n","\r\n","    # train or validate on IAM dataset\r\n","    if args_train or args_validate:\r\n","        # load training data, create TF model\r\n","        loader = DataLoaderIAM(args_data_dir, args_batch_size, Model.imgSize, Model.maxTextLen, args_fast)\r\n","\r\n","        # save characters of model for inference mode\r\n","        open(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\r\n","\r\n","        # save words contained in dataset into file\r\n","        open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\r\n","\r\n","        # execute training or validation\r\n","        if args_train:\r\n","            model = Model(loader.charList, decoderType)\r\n","            train(model, loader)\r\n","        elif args_validate:\r\n","            model = Model(loader.charList, decoderType, mustRestore=True)\r\n","            validate(model, loader)\r\n","\r\n","    # infer text on test image\r\n","    else:\r\n","        model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args_dump)\r\n","        infer(model, FilePaths.fnInfer)\r\n","\r\n","\r\n","if __name__ == '__main__':\r\n","    main()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wfgwWfGgp_3"},"source":["!zip -r model_trained.zip ../data ../model\r\n","\r\n","!cp 'model_trained.zip' '/content/drive/MyDrive/TFG/TrainSimpleHTR/model_trained.zip'"],"execution_count":null,"outputs":[]}]}